<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: django | David Cramer's Blog]]></title>
  <link href="http://justcramer.com/categories/django/atom.xml" rel="self"/>
  <link href="http://justcramer.com/"/>
  <updated>2012-08-30T21:53:16-07:00</updated>
  <id>http://justcramer.com/</id>
  <author>
    <name><![CDATA[David Cramer]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How 'NoOps' Works for Sentry]]></title>
    <link href="http://justcramer.com/2012/08/30/how-noops-works-for-sentry"/>
    <updated>2012-08-30T20:59:00-07:00</updated>
    <id>http://justcramer.com/2012/08/30/how-noops-works-for-sentry</id>
    <content type="html"><![CDATA[<p>I've talked a lot about how I run getsentry.com, mostly with <a href="http://justcramer.com/2012/06/02/the-cloud-is-not-for-you/">my experiences
on Heroku</a> and how I <a href="http://justcramer.com/2012/06/03/scaling-your-clouds/">switched to leased servers</a>. Many people consistently
suggested that operations work is difficult so they shouldn't deal with it themselves. I'm not going to tell you that my
roommate, <a href="http://twitter.com/drunkdev">Mike Clarke</a>, who is also one of the few operations people we have at
<a href="http://disqus.com">DISQUS</a> has it easy, but I'd like to give you a little bit of food for thought.</p>




<p>GetSentry started around Christmas of 2011. I had already built and open sourced Sentry at Disqus, and the idea was to
    take that work and create a Heroku AddOn out of it. The pitch was that I could make a little bit of money on the side
    simply by hosting Sentry for people. About three months later I had that prototype hosting service running on Heroku,
    accepting payments both via the AddOn infrastructure, as well as on my own using the amazing
    <a href="http://stripe.com">Stripe</a> platform.</p>




<p>Let's fast forward to today. I no longer host anything on Heroku (or any cloud provider), and instead I lease servers.
    Now the company I lease from is what most people would call a "budget provider". They're extremely cheap (they dont
    take extreme margins off the cost of the machines you're leasing), and they do absolutely nothing for you. It's not
    for the faint of heart. That said, it's also how I can get away with very low costs.</p>




<p>I'm going to tell you a bit of a story of how I switched from Heroku to fully configured leased servers in less than
    a week, in my free time. I'm also going to try to convince you that it's really <strong>not that complicated</strong>.</p>




<h3>The First Server</h3>




<p>This part could be more appropriately titled "Learning Chef". I'm fortunate to have some awesome coworkers, and even
    more fortunate that when I was making this transitiong I had access to my roommate to prod him about questions. I'm
    also extremely fortunate that medians like Google, IRC, and Twitter exist for any other questions I ever have.

<p>The first task I had to getting my prototype web server online was to get it all configured. I could have taken the
    old fashioned approach of creating a few config files locally (vcs maybe) and then sending them up to the server,
    as well as manually installing whatever packages I needed (nginx, memcache, etc.), but with Puppet and Chef becoming
    all the range I figured it was as good as time as ever to dig into one.</p>

<p>I decided to use the Chef hosted service, and after a few bumps with figuring out what all this Ruby stuff was about,
    I had managed to get a basic understanding of roles and cookbooks. After quite a bit of fiddling I had created a
    cookbook specific to getsentry (which holds things like setting up varoius paths), and a bunch of generic ones,
    like apt, nginx, memcached, python, etc.</p>

<h3>Creating a Recipe</h3>

<p>The meat of this was handled via Chef's awesome roles, and wiring up a few things in the 'default' recipe of getsentry:</p>

<pre>include_recipe "python"

directory "/srv/www" do
  owner "root"
  group "root"
  mode "0755"
  action :create
end

directory "/srv/www/getsentry.com" do
  owner "dcramer"
  group "dcramer"
  mode "0755"
  action :create
end
</pre>

<p>This formed the basis of any server that I would be running, and simply setup a couple of directories. I also simply
    gave ownership to my user, as I'm the only one working on the project, and didn't need the added complexities of build
    or system users.</p>

<p>I then moved on to a second recipe, which formed the basis of a web node. This one has a lot more to it, as it needed
    to configure nginx and memcache at the start:</p>

<pre>include_recipe "getsentry"
include_recipe "supervisor"

template "#{node[:nginx][:dir]}/sites-available/getsentry.com" do
  source "nginx/getsentry.erb"
  owner "root"
  group "root"
  mode 0644
  notifies :reload, "service[nginx]"
end

nginx_site "getsentry.com"

supervisor_service "web-1" do
  directory "/srv/www/getsentry.com/current/"
  command "/srv/www/getsentry.com/env/bin/python manage.py run_gunicorn -b 0.0.0.0:9000 -w #{node[:getsentry][:web][:workers]}"
  environment "DJANGO_CONF" => node[:django_conf]
  user "dcramer"
end

supervisor_service "web-2" do
  directory "/srv/www/getsentry.com/current/"
  command "/srv/www/getsentry.com/env/bin/python manage.py run_gunicorn -b 0.0.0.0:9001 -w #{node[:getsentry][:web][:workers]}"
  environment "DJANGO_CONF" => node[:django_conf]
  user "dcramer"
end</pre>

<p>There is a bit more to it then what I've shown, but all in all it was pretty simple. It just took me a bit to understand
    how chef functioned. All in all, I'm now an engineer that has experience in Chef, even if it's very little, which
    from my perspective (on the hiring end at Disqus), is an extremely nice addition to a skillset.</p>

<p>Once the web server was online, all I had to do was to configure a primary database server. I simply brought up another
    node, gave it a new role (db), and didn't even need to create a custom recipe (I simply reused the existing pgbouncer,
    postgersql, and redis recipes available elsewhere on the internet).</p>

<h3>Operational Complexity</h3>

<p>I stated in the beginning that I completed this process in less than a week. From Heroku to hardware it took me about
    three evenings of toying with Chef (mostly more complex components, like iptables and building a deploy script). What
    I really want to point out is how I have <strong>never</strong> been in an operations position. I've definitely configured
    servers (ala apt-get install nano), and know my way around, especially with a database, but most of this was fairly
    new to me.</p>

<p>The continued argument of it being "too difficult" to run your own servers is quite the overstatement. There are many
    things I have to be concerned about, most importantly data loss and the ability to recover in the event of a disaster
    on my machines. These also aren't overly complex challenges to handle.</p>

<p>Data redundancy is handled a simple cron script that does nightly backups to S3. It's literally just a script that calls
    pg_dump and s3cmd to send the files upstream. Now that's not enough for any real requirements, so step two is simply
    setting up replication on your database node to a second server, if if that server is your application server.</p>

<p>Availability is the second big problem, and is easily avoided the same way that you avoid losing your database: have
    a second server. This again can be a server thats primary task is for something other than your application (it can
    be your database). It doesnt have to a permanent location for it. It only has to survive until a primary server is
    available or you're willing and able to invest in more hardware.</p>

<h3>Closing Thoughts</h3>

<p>I spent an initial three evenings, and another week's worth since on server configuring an operations. There were
    various problems like Postgres not being tuned well enough (pgtune is amazing by the way), DNS being slow (fuck it,
    use IPs), and some more minor things that needed addressed throughout that time. All in all, there's basically
    zero day-to-day operations concerns, and most of the work happens when I need to expand the system (which is rare).</p>

<p>All of it ended as an extremely valuable learning experience, but you using Chef wasn't a necessity. I could have done
    things the more "amateur" way, but I also now have the benefit of being able to bring online a server, run a few
    commands, and have a machine or even a cluster identical to what's already running.</p>

<p>On the limited hardware I run for getsentry.com, that is, two servers that actually service requests (one database,
    one app), we've serviced around 25 million requests since August 1st, doing anywhere from 500k to 2 million in a
    single day. That isn't that much traffic, but what's important is it services those requests very quickly, and is
    using very little of the resources that are dedicated to it. In the end, this means that Sentry's revenue will grow
    much more quickly than it's monthly bill will.</p>

<p>GetSentry has been profitable since its 4th month, and currently only spends 10% of its monthly revenue (hardware and
    other third party services). That gap gets larger every month, and I've been more than happy to invest some of my
    time to keep that gap as large as possible.</p>

<p>Also, <a href="http://whoownsmyavailability.com/">this</a>.</p>

<p>Look for a future post with many more details on how I setup Chef (likely incorrect) with more in-depth code and
    configuration from the cookbooks.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scaling Your Clouds]]></title>
    <link href="http://justcramer.com/2012/06/03/scaling-your-clouds"/>
    <updated>2012-06-03T17:53:00-07:00</updated>
    <id>http://justcramer.com/2012/06/03/scaling-your-clouds</id>
    <content type="html"><![CDATA[<p>My <a href="http://justcramer.com/2012/06/02/the-cloud-is-not-for-you/">post</a> yesterday seems to have gotten all the cloud fanboy's panties into a twist, so I figured I'd give them something
else to rage about.</p>




<p>There were lots of claims that without the cloud you can't scale, or you dont have redundancy, or you can't come up
with the result of <em>2 + 2</em>. I can't even explain the level of ignorance I've seen come out of the woodwork.

<p>So let's clarify some things.</p>

<h3>"The Cloud"</h3>

<p>There are many definitions that float around for "the cloud", and what it means, and more specifically what it's
supposed to do for you. When I talk about it, I'm not talking about you setting up hundreds of your own servers and
virtualizing them. <strong>We do that too.</strong> I'm talking about the notion that there's some mythical provider
that is going to cater to your needs and you're never going to have to worry about operational concerns.</p>

<p>There is nothing wrong with using Heroku, AWS, Dotcloud, or any of the hundreds of other cloud providers out there.
They all provide you with some level of relaxed operational requirements. That said, you're still restricted to whatever
completely fucking shit hardware they decide is right for virtualization. Now I'm not talking AWS so much, as they do
allow reasonable size instances, but you're still restricted to what they're willing to offer. You never
have the option to order custom hardware.</p>

<h3>Scale</h3>

<p>A bunch of the internet hipsters on Hacker News and elsewhere seem to think that if you use the cloud, your application
is going to magically scale by adding more servers to it. That may be true if you're using MongoDB, but we dont
live in a fairy tale here and it will not ever work. There are very few systems that I'm aware
of that can scale from one machine to tens to hundreds to thousands without a massive rearchitecture of how you use the
system.</p>

<p>One of the first things I pointed out in my article was the fact that I had to spin up large amounts of instances to
handle temporary workload. Too bad the database was bottlenecking on concurrent writes to the same row. You can't ignore one important factor:
I cant just "spin up more database". There are many amazing systems out there that are built
on the notion of distributed data with the goal of some level of horizontal scalability (<a href="http://basho.com/products/riak-overview/">Riak</a>,
<a href="http://cassandra.apache.org/">Cassandra</a>). Even they also do not allow you to spin up more servers and gain more capacity immediately.</p>

<h3>Operations Complexity</h3>

<p>Another argument that was brought up was the fact that I now personally have to deal with redundancy, monitoring, security
fixes, OS upgrades, bringing up more servers, etc.. Sure, that's true. Except that that will cost me far less time than I would
have spent trying to create a SQL database that can horizontal scale to infinity.</p>

<ul>
    <li>Redundancy is easy, especially at small scale. Cloud hosting is not going to solve your database redundancy for you.</li>
    <li>Just because I'm hosting my own machines doesnt mean I cant use New Relic, or in my case <a href="http://scoutapp.com">Scout</a>.</li>
    <li>I dont need to frequently bring up additional servers to handle the load because my actual hardware performs 2000 times better than my old virtualized hardware</li>
    <li>Security updates? OS reloads? Its not like I'm compiling shit by hand, and through the convenience of configuration management this is unbelievably easy.</li>
</ul>

<p>If you ignore the entirety of operations, you will never have any idea what's going on when there's a problem.</p>

<h3>The Time/Cost Tradeoff</h3>

<p>In my original post I stated it took me about three days to get everything into Chef, and have the new hardware ordered and online. Even if this was three full days of my time, I had just spent four days a previous week trying to get the infinitely scalable cloud solution to perform well enough. Simple math right, four is more than three. <strong>Not worth it.</strong></p>

<p>I built <a href="https://www.getsentry.com">getsentry.com</a> specifically with the goal of optimizing cost vs
profit margins. Ths is the first month that it's been profitable, and unless every single customer jumps ship at once,
it's unlikely that I will ever have to put my own money (excluding my time) into the project again.</p>

<h3>tl;dr</h3>

<p>Virtualized computing has many great uses, but you do not <strong>need</strong> it, especially if you're just starting
a business. If you want to try out a provider, don't let me stop you. Make your own decisions. That said, you can be <em>anything</em>
at <em>any random company</em> and tell me you use the cloud successfully, and I'll give you a pat on the back. I'll then
tell you that we rent servers successfully, and by we, I mean <a href="http://disqus.com">DISQUS</a>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Cloud is Not For You]]></title>
    <link href="http://justcramer.com/2012/06/02/the-cloud-is-not-for-you"/>
    <updated>2012-06-02T13:57:00-07:00</updated>
    <id>http://justcramer.com/2012/06/02/the-cloud-is-not-for-you</id>
    <content type="html"><![CDATA[<p><strong>Update:</strong> Did I hurt your feelings with this post? Read
<a href="http://justcramer.com/2012/06/03/scaling-your-clouds/">Scaling your Clouds</a> so you can rage even more.</p>




<p>Well, maybe not specifically <em>you</em>, but the mass that screams it will solve their problems.</p>




<p>It's been a fun year so far. There's been exciting things happening both for me personally, as well as at DISQUS. One of
those things has been launching a new side project, <a href="https://www.getsentry.com">getsentry.com</a>. This is about
it's 4th month running publicly, and it's been doing very well. I wanted to talk a little bit about where it started, and
how quickly it's shifted in the stance of where it's going.</p>




<p>Around Christmas of 2011, and after a lot of prodding by <a href="http://www.craigkerstiens.com/">Craig Kerstiens</a> (of Heroku)
I had finally given in to the pressure of creating a hosted version of Sentry to launch as a Heroku addon. I already knew
Sentry was awesome, as did many others, and this just meant getting something I put a lot of effort into out in front of
so many others. It was very little work to get things up and running on Heroku, and just as easy to setup the addon
endpoints. We started a private beta shortly thereafter, and immediately picked up a bunch of the Django/Python crowd.</p>




<p>From there it slowly, but steadily grew in both customers and data. In fact, for the first couple of months we were able
to survive on just a few dynos and the first tier of dedicated postgres (which was the $200 package at the time). We've
also expanded to cover nearly all popular languages, including PHP, Ruby, Java, and even JavaScript.</p>




<p>A bit further in the background of how I structured the Sentry service:</p>




<ul>
    <li>Two separate apps (www and app)</li>
    <li>SSL everywhere (two certs, two addons, $40/month plus SSL cert costs)</li>
    <li>A minimum of two dynos each ($72/month~)</li>
    <li>Tier-1 dedicated DB (Ronin, $200/month)</li>
</ul>




<p>Now, before I continue, let me say that I thoroughly enjoyed using Heroku. It's a great service, I'm friends with a lot
of people there. That said, I want to explain why you shouldn't use Heroku, or the cloud. Let me also clarify that I'm not
talking about the limitations of the idea of the cloud, but more specifically the limitations I've seen from providers,
and specifically my experience with Heroku.</p>




<p>Right from the get-go we had a system that had pretty good HA and redundancy, especially due to how Heroku's Postgres
solution works. Unfortunately, we quickly saw the limitations of what both the Postgres and the dynos could handle.</p>




<p>Our first attempt to address this was to add worker nodes (ala Celery) to handle concurrency better. This turned into one
or two additional dynos dedicated to processing jobs, as well as an additional Redis addon. Unfortunately the Redis addon
is completely overpriced, we quickly shifted to pulling up a VM in Linode's eastcoast datacenter instead. This bought us
a little bit of time, but really I'd say we were only given an additional 10% capacity by what should have been a large
optimization.</p>




<p>Another week or two went by, and it was suggested that we get off the Ronin database, and upgrade to the Fugu package (
which bumped up the database cost to $400/month). This did quite a bit. In fact, this let us handle most things without
too much of a concern. A little while down the road, we had a customer sign up who was actually send realistic amounts of
data. More specifically, <strong>not even close to the amount of data Disqus' Sentry server handles</strong>, but about 10x
more than the rest of our customers combined had been sending.</p>




<p>Then shit started to hit the fan.</p>




<p>In no specific order, we started finding numerous problems with various systems:</p>




<ul>
    <li>Redis takes too much memory to reliably queue Sentry jobs.</li>
    <li>Dynos are either memory or CPU bound, but we have no idea how or why.</li>
    <li>The Postgres server can't handle any reasonable level of concurrency.</li>
    <li>We randomly have to spin up 20 dynos to get anywhere in the queue backlog.</li>
</ul>




<p>Given all of that, I made the decision that I was going to go back to using real hardware and managing it myself. I'm
no stranger to operations work, though it's never been my day job. I did however want to do this right, and with the advice
of my coworker, friend, and roommate, <a href="https://twitter.com/#!/sugarc0de">Mike Clarke</a> I decided I'd set these
up properly, with Chef.</p>




<p>About three days into it, and I had learned how to use Chef (I don't write Ruby), brought up two full pluggable
configurations for a db node and a web node, written a deployment script in Fabric, migrated to the new hardware and
destroyed my Heroku and Linode instances. Three days, that's all it took to replace the cloud.</p>




<p>Now you might argue that the cloud let's you scale up easily. <strong>YOU ARE WRONG, IT DOES NOT.</strong> The cloud
gives you the convenience, or more importantly, the illusion of convenience, that you can bring up nodes to add to your
network without giving it much thought. You can do that. You don't ever realistically need to do that.</p>




<p>Almost any company worth a damn can bring online a server within 24 hours, even budget companies. When have you actually
needed turnaround time faster than that? If you did, maybe you should read up on capacity planning.</p>




<p>The hosted Sentry now runs on two budget servers, one of which runs Postgres, pgbouncer, and Redis, the other handles
Nginx, Celery, memcached, and the Python webserver. The cost for these two machines? About $300/month. When I destroyed
Heroku, my bill was looking to be around $600-700 between Heroku and Linode. Given the numbers we run at Disqus, the
physical hardware should be able to handle no less than 2000% the capacity I was struggling to handle on the cloud.</p>




<p>I'm not saying you can't make use of the cloud. For example, Disqus uses Amazon for running large amounts of map/reduce
work. You know, <strong>elastic computing</strong>, the kind of computing that is inconsistent, unplanned, or generally
infrequent. I'm also not saying you shouldn't use Heroku. You should see if it works for you. However, if you ever come up
to me and argue that the cloud is going to fix any problem, I'll make the assumption that you're one of those annoying
kids that runs around screaming MongoDB and Node.js are the answer to all of the worlds problems.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Distributing Work in Python Without Celery]]></title>
    <link href="http://justcramer.com/2012/05/04/distributing-work-without-celery"/>
    <updated>2012-05-04T15:12:00-07:00</updated>
    <id>http://justcramer.com/2012/05/04/distributing-work-without-celery</id>
    <content type="html"><![CDATA[<p>We've been migrating a lot of data to various places lately at DISQUS. These generally have been things like running
consistancy checks on our PostgreSQL shards, or creating a new system which requires a certain form of denormalized data. It
usually involves iterating through the results of an entire table (and sometimes even more), and performing some action
based on that row. We never care about results, we just want to be able to finish as quickly as possible.</p>




<p>Generally, we'd just create a simple <code>do_something.py</code> that would look something like this:</p>


<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">for</span> <span class="n">comment</span> <span class="ow">in</span> <span class="n">RangeQuerySetWrapper</span><span class="p">(</span><span class="n">Post</span><span class="o">.</span><span class="n">objects</span><span class="o">.</span><span class="n">all</span><span class="p">()):</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">do_something</span><span class="p">(</span><span class="n">comment</span><span class="p">)</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Note: RangeQuerySetWrapper is a wrapper around Django's ORM that efficiently iterates a table.</p>




<p>Eventually we came up with an internal tool to make this a bit more bearable. Mostly to handle resuming processes based
on the last primary key, and to track status. It evolved into a slightly more complex, but still simple utility we called
Taskmaster:</p>


<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">def</span> <span class="nf">callback</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">do_something</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="o">**</span><span class="n">options</span><span class="p">):</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">qs</span> <span class="o">=</span> <span class="n">Post</span><span class="o">.</span><span class="n">objects</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</span><span class='line'><span class="n">tm</span> <span class="o">=</span> <span class="n">Taskmaster</span><span class="p">(</span><span class="n">callback</span><span class="p">,</span> <span class="n">qs</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">)</span>
</span><span class='line'><span class="n">tm</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>This used to never be much of a problem. We'd just spin up some utility server and max the CPUs on that single machine
to get data processed in a day or less. Lately however, we've grown beyond the bounds of what is reasonable for a single
machine to take care of, and we've had to look towards other solutions.</p>




<h3>Why Not Celery?</h3>




<p>As with most people, we rely on Celery and RabbitMQ for distributing asyncrhonous tasks in our application. Unfortunately
that's not quite the ideal fit out of the box for us in these situations. The root of the problem stems from the fact that
we may need to run through a billion objects, and without some effort, that would mean every single task would need to
fit into a RabbitMQ instance.</p>




<p>Given that we can't simply queue every task and then distribute them to some Celery workers, and even more so that we
simply dont want to bring up Celery machines/write throwaway Celery code for a simple script, we chose to take a different
route. That route ended up with a simple distributed buffer queue, built on the 
<a href="http://docs.python.org/library/multiprocessing.html">Python multiprocessing module</a>.</p>




<h3>Introducing Taskmaster</h3>




<p><a href="https://github.com/dcramer/taskmaster">Taskmaster</a> takes advantage of the remote management capabilities built into the multiprocessing module. This makes it
very simple to just throw in a capped Queue and have workers connect, get and execute jobs, and control state via that
single master process. In the end, we came up with an API looking something like this:</p>


<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;spawn the master process&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;<span class="nv">$ </span>tm-master taskmaster.example --reset --key<span class="o">=</span>foo --host<span class="o">=</span>0.0.0.0:5050&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;run a slave&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;<span class="nv">$ </span>tm-slave do_something:handle_job --host<span class="o">=</span>192.168.0.1:5050
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>You'll see the status on the master as things process, and if you cancel the process and start it again, it will
automatically resume:</p>


<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>tm-master taskmaster.example --reset --key<span class="o">=</span>foo --host<span class="o">=</span>0.0.0.0:5050
</span><span class='line'>Taskmaster server running on <span class="s1">&#39;0.0.0.0:5050&#39;</span>
</span><span class='line'>Current Job: 30421 | Rate:  991.06/s | Elapsed Time: 0:00:40
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Implementing the iterator and the callback are just as simple as they used to be:</p>


<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">def</span> <span class="nf">get_jobs</span><span class="p">(</span><span class="n">last</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="c"># ``last`` will only be passed if previous state was available</span>
</span><span class='line'><span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">RangeQuerySetWrapper</span><span class="p">(</span><span class="n">Post</span><span class="o">.</span><span class="n">objects</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="n">min_id</span><span class="o">=</span><span class="n">last</span><span class="p">):</span>
</span><span class='line'>    <span class="k">yield</span> <span class="n">obj</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">def</span> <span class="nf">handle_job</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="k">print</span> <span class="s">&quot;Got </span><span class="si">%r</span><span class="s">!&quot;</span> <span class="o">%</span> <span class="n">obj</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Now under the hood Taskmaster will continue to iterate on <code>get_jobs</code> whenever the size of the queue is
under the threshold (which defaults to 10,000 items). This means we have a constant memory footprint and can just spin
slaves to process the data.</p>




<p>Taskmaster is still new, but if you're in need of these kinds of one-off migration scripts, we encourage you to <a href="https://github.com/dcramer/taskmaster">try
it out</a> and see if it fits.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Travis-CI with Python and Django]]></title>
    <link href="http://justcramer.com/2012/05/03/using-travis-ci"/>
    <updated>2012-05-03T11:13:00-07:00</updated>
    <id>http://justcramer.com/2012/05/03/using-travis-ci</id>
    <content type="html"><![CDATA[<p>I've been using <a href="http://travis-ci.org">Travis-CI</a> for a while now. Both my personal projects,
and even several of the libraries we maintain at DISQUS rely on it for Continuous Integration. I figured it was about time to confess
my undenying love for Travis, and throw up some notes about the defaults we use in our projects.</p>




<p>Getting started with Travis-CI is pretty easy. It involves putting a <code>.travis.yml</code> file in the root of
your project, and configuring the hooks between GitHub and Travis. While it's not always easy to get the hooks configured
when you're using organizations, I'm not going to talk much about that. What I do want to share is how we've structured
our configuration files for our Django and Python projects.</p>




<p>A basic <code>.travis.yml</code> might look something like this:</p>


<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">language</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">python</span>
</span><span class='line'><span class="l-Scalar-Plain">python</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="s">&quot;2.6&quot;</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="s">&quot;2.7&quot;</span>
</span><span class='line'><span class="l-Scalar-Plain">install</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">pip install -q -e . --use-mirrors</span>
</span><span class='line'><span class="l-Scalar-Plain">script</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">python setup.py test</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Most of the projects themselves use Django, which also means they need to test several Django versions. Travis makes
this very simple with its matrix builds. In our case, we need to setup a DJANGO matrix, and ensure it gets installed:</p>


<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">env</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">DJANGO=1.2.7</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">DJANGO=1.3.1</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">DJANGO=1.4</span>
</span><span class='line'><span class="l-Scalar-Plain">install</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">pip install -q Django==$DJANGO --use-mirrors</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">pip install -q -e . --use-mirrors</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Additionally we generally conform to pep8, and we always want to run pyflakes against our build. We also use a custom
version of pyflakes which allows us to filter out warnings, as those are never critical errors. Add this in is pretty
simple using the <code>before_script</code> hook, which gets run before the tests are run in <code>script</code>.</p>


<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">install</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">pip install -q Django==$DJANGO --use-mirrors</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">pip install pep8 --use-mirrors</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">pip install https://github.com/dcramer/pyflakes/tarball/master</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">pip install -q -e . --use-mirrors</span>
</span><span class='line'><span class="l-Scalar-Plain">before_script</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="s">&quot;pep8</span><span class="nv"> </span><span class="s">--exclude=migrations</span><span class="nv"> </span><span class="s">--ignore=E501,E225</span><span class="nv"> </span><span class="s">src&quot;</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">pyflakes -x W src</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>When all is said and done, we end up with something like this:</p>


<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">language</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">python</span>
</span><span class='line'><span class="l-Scalar-Plain">python</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="s">&quot;2.6&quot;</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="s">&quot;2.7&quot;</span>
</span><span class='line'><span class="l-Scalar-Plain">env</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">DJANGO=1.2.7</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">DJANGO=1.3.1</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">DJANGO=1.4</span>
</span><span class='line'><span class="l-Scalar-Plain">install</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">pip install -q Django==$DJANGO --use-mirrors</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">pip install pep8 --use-mirrors</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">pip install https://github.com/dcramer/pyflakes/tarball/master</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">pip install -q -e . --use-mirrors</span>
</span><span class='line'><span class="l-Scalar-Plain">before_script</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="s">&quot;pep8</span><span class="nv"> </span><span class="s">--exclude=migrations</span><span class="nv"> </span><span class="s">--ignore=E501,E225</span><span class="nv"> </span><span class="s">src&quot;</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">pyflakes -x W src</span>
</span><span class='line'><span class="l-Scalar-Plain">script</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">python setup.py test</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Travis will automatically matrix each environment variable with each Python version, so you'll get
a test run for every combination of the two. Pretty easy, right?</p>

]]></content>
  </entry>
  
</feed>
