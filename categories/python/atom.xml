<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: python | David Cramer's Blog]]></title>
  <link href="http://justcramer.com/categories/python/atom.xml" rel="self"/>
  <link href="http://justcramer.com/"/>
  <updated>2012-05-03T11:25:42-07:00</updated>
  <id>http://justcramer.com/</id>
  <author>
    <name><![CDATA[David Cramer]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Using Travis-CI with Python and Django]]></title>
    <link href="http://justcramer.com/2012/05/03/using-travis-ci"/>
    <updated>2012-05-03T11:13:00-07:00</updated>
    <id>http://justcramer.com/2012/05/03/using-travis-ci</id>
    <content type="html"><![CDATA[<p>I've been using <a href="http://travis-ci.org">Travis-CI</a> for a few weeks now. Both my personal projects,
and even several of the libraries we maintain at DISQUS rely on it for Continuous Integration. I figured it was about time to confess
my undenying love for Travis, and throw up some notes about the defaults we use in our projects.</p>




<p>Getting started with Travis-CI is pretty easy. It involves putting a <code>.travis.yml</code> file in the root of
your project, and configuring the hooks between GitHub and Travis. While it's not always easy to get the hooks configured
when you're using organizations, I'm not going to talk much about that. What I do want to share is how we've structured
our configuration files for our Django and Python projects.</p>




<p>A basic <code>.travis.yml</code> might look something like this:</p>


<p>{% codeblock lang:yaml %}
language: python
python:
  - "2.6"
  - "2.7"
install:
  - pip install -q -e . --use-mirrors
script:
  - python setup.py test
{% endcodeblock %}</p>

<p>Most of the projects themselves use Django, which also means they need to test several Django versions. Travis makes
this very simple with its matrix builds. In our case, we need to setup a DJANGO matrix, and ensure it gets installed:</p>


<p>{% codeblock lang:yaml %}
env:
  - DJANGO=1.2.7
  - DJANGO=1.3.1
  - DJANGO=1.4
install:
  - pip install -q Django==$DJANGO --use-mirrors
  - pip install -q -e . --use-mirrors
{% endcodeblock %}</p>

<p>Additionally we generally conform to pep8, and we always want to run pyflakes against our build. We use a custom
version of pyflakes which allows us to filter out warnings, as those are never critical errors:</p>


<p>{% codeblock lang:yaml %}
install:
  - pip install -q Django==$DJANGO --use-mirrors
  - pip install pep8 --use-mirrors
  - pip install https://github.com/dcramer/pyflakes/tarball/master
  - pip install -q -e . --use-mirrors
script:
  - "pep8 --exclude=migrations --ignore=E501,E225 src"
  - pyflakes -x W src
  - python setup.py test
{% endcodeblock %}</p>

<p>When all is said and done, we end up with something like this:</p>


<p>{% codeblock lang:yaml %}
language: python
python:
  - "2.6"
  - "2.7"
env:
  - DJANGO=1.2.7
  - DJANGO=1.3.1
  - DJANGO=1.4
install:
  - pip install -q Django==$DJANGO --use-mirrors
  - pip install pep8 --use-mirrors
  - pip install https://github.com/dcramer/pyflakes/tarball/master
  - pip install -q -e . --use-mirrors
script:
  - "pep8 --exclude=migrations --ignore=E501,E225 src"
  - pyflakes -x W src
  - python setup.py test
{% endcodeblock %}</p>

<p>Travis will automatically matrix each environment variable with each Python version, so you'll get
a test run for every combination of the two. Pretty easy, right?</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sticking With Standards]]></title>
    <link href="http://justcramer.com/2012/04/24/sticking-with-standards"/>
    <updated>2012-04-24T22:23:00-07:00</updated>
    <id>http://justcramer.com/2012/04/24/sticking-with-standards</id>
    <content type="html"><![CDATA[<p>More and more I'm seeing the "requirements.txt pattern" come up. This generally refers to projects (but not just), and
seems to have started around the same time as Heroku adopting Python. I feel like this is something that matters in the
Python world, and because I have an opinion on everything, I want to share mine.</p>




<h3>requirements.txt</h3>




<p>Let's first talk about what this pattern actually is. As you should already be familiar with pip (if you're not, this
post is not for you), the idea of this is that whatever you're doing, is installable by pointing pip at a requirements.txt
file which contains a list of your projects dependencies. This has some obvious benefits, one being that you can
mark repositories as dependencies.</p>




<p>Another benefit of this is when you have a large project (like DISQUS) and your dependencies can vary between environments. For
example, we have several various requirements files for disqus-web (our largest package):</p>




<pre>
requirements/global.txt
requirements/production.txt
requirements/development.txt
</pre>




<p>These end up being pretty obvious, and when an app has specific needs there's no reason not to approach the problem this
way. That said, you dont <strong>need</strong> to do things this way, and in every project other than our main repository,
including our open source work, all dependencies are specified completely in setup.py. Even in this case, we could just
as easily specify our core requirements as part of the package and simply have additional files which label the production
and development dependencies.</p>




<h3>setup.py is the right choice</h3>




<p>A common argument for not using setup.py is that a library is not the same as an app (or larger project). Why not? We
employ the same metadata in everything. Each contains a list of dependencies, some various metadata, and possibly a list
of extra resources (such as scripts, or documentation). Fundamentally they're identical. Additionally, if pip is your
thing, it <strong>does not prevent you from using setup.py</strong>. Let's take an example setup.py:</p>


<p>{% codeblock lang:python %}
from setuptools import setup, find_packages</p>

<p>requires = [</p>

<pre><code>'Flask==0.8',
'redis==2.4.11',
'hiredis==0.1.1',
'nydus==0.8.1',
</code></pre>

<p>]</p>

<p>setup(</p>

<pre><code>name='something-sexy',
version='1.0.0',
author="DISQUS",
author_email="dev@disqus.com",
package_dir={'': 'src'},
packages=find_packages("src"),
install_requires=requires,
zip_safe=False,
</code></pre>

<p>)
{% endcodeblock %}</p>

<p>Now, in our case, this is probably a service on Disqus, which means we're not listing it as a dependancy. In every
single scenario we have, we want our package to be on <code>PYTHONPATH</code>, and this is no different. There's many ways
to solve the problem, and generally adjusting <code>sys.path</code> is not what you're going to want. Whether you install
the package or you just run it as an editable package (via pip install -e or setuptool's develop command), packaging
your app makes it that much easier.</p>




<p>What's even more important is that you <strong>stick with standards</strong>, especially in our growing ecosystem of
open source and widely available libraries. There's absolutely no reason to have to explain to a developer that they
need to run some arbitrary command to get your neat tool to install. Following the well defined and adopted standards
ensures that is never the case.</p>




<p>Keep it simple. Keep it obvious.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Integrating Django with Nose at DISQUS]]></title>
    <link href="http://justcramer.com/2011/08/05/extending-django-nose"/>
    <updated>2011-08-05T00:00:00-07:00</updated>
    <id>http://justcramer.com/2011/08/05/extending-django-nose</id>
    <content type="html"><![CDATA[<p>About a month ago we decided to make the transition off of Django's test suite over to the Nose runners. Our main selling point was the extensibility, and the existing ecosystem of plugins. Four weeks later I'm happy to say we're running (basically) Nose with some minor extensions, and it's working great.</p>




<p>Getting Django running on Nose is no small feat. Luckily, someone else has already put in a lot of that effort, and packaged it up all nice and neat as <a href="http://pypi.python.org/pypi/django-nose">django-nose</a>. I won't go through setting up the package, but it's pretty straight forward. One thing that we quickly noticed however, was that it didnt quite fit our approach to testing, which was strictly unittest. After a couple days of going back and forth with some minor issues, we came up with a few pretty useful extensions to the platform.</p>




<p>A few of the big highlights for us:</p>




<ul>
    <li>Xunit integration (XML output of test results)</li>
    <li>Skipped and deprecated test hooks</li>
    <li>The ability to organize tests outside of the Django standards</li>
</ul>




<p>I'm wanted to talk a bit about how we solved some of our problems, and the other benefits we've seen since adopting it.</p>




<h3>Test Organization</h3>




<p>The biggest win for us was definitely being able to reorganize our test suite. This took a bit of work, and I'll talk about this with some of the plugins we whipped up to solve the problems. We ended up with a nice extensible test structure, similar to Django's own test suite:</p>




<pre>
tests/
tests/db/
tests/db/connections/
tests/db/connections/redis/
tests/db/connections/redis/__init__.py
tests/db/connections/redis/models.py
tests/db/connections/redis/tests.py
</pre>




<p>We retained the ability to keep tests within the common <code>app/tests</code> convention, but we found that we were just stuffing too many tests into obscure application paths that it became unmaintainable after a while.</p>


<h3>Unittest Compatibility</h3>




<p>The first issue we hit was with test discovery. Nose has a pretty good default pattern for finding tests, but it had some behavior that didn't quite fit with all of our existing code. Mostly, it found random functions that were prefixed with <code>test_</code>, or things like <code>start_test_server</code> which weren't tests by themselves.</p>




<p>After digging a bit into the API, it turned out to be a pretty easy problem to solve, and we came up with the following plugin:</p>


<p>{% codeblock lang:python %}
class UnitTestPlugin(object):</p>

<pre><code>"""
Enables unittest compatibility mode (dont test functions, only TestCase
subclasses, and only methods that start with [Tt]est).
"""
enabled = True

def wantClass(self, cls):
    if not issubclass(cls, unittest.TestCase):
        return False

def wantMethod(self, method):
    if not issubclass(method.im_class, unittest.TestCase):
        return False
    if not method.__name__.lower().startswith('test'):
        return False

def wantFunction(self, function):
    return False
</code></pre>

<p>{% endcodeblock %}</p>

<h2>Test Case Selection</h2>




<p>To ensure compatibility with our previous unittest extensions, we needed a simple way to filter only selenium tests. We do this with the --selenium and --exclude-selenium flags.</p>


<p>{% codeblock lang:python %}
from disqus.tests.testcases import DisqusSeleniumTest
from nose.plugins.base import Plugin</p>

<p>class SeleniumSelector(Plugin):</p>

<pre><code>def options(self, parser, env):
    parser.add_option("--exclude-selenium",
                      dest="selenium", action="store_false",
                      default=None)
    parser.add_option("--selenium",
                      dest="selenium", action="store_true",
                      default=None)

def configure(self, options, config):
    self.selenium = options.selenium
    self.enabled = options.selenium is not None

def wantClass(self, cls):
    if self.selenium:
        return issubclass(cls, DisqusSeleniumTest)
    elif issubclass(cls, DisqusSeleniumTest):
        return False
</code></pre>

<p>{% endcodeblock %}</p>

<h2>Bisecting Tests</h2>




<p>One feature I always thought was pretty useful in the Django test suite was their <code>--bisect</code> flag. Basically, given your test suite, and a failing test, it could help you find failures which were related to executing tests in say a specific order. This isn't actually made available to normal Django applications, but being a large codebase it's extremely useful for us.</p>




<p><strong>I should note, this one adapted from Django and is very rough. It doesn't report a proper <code>TestResult</code>, but it's pretty close to where we want to get it.</strong></p>


<p>{% codeblock lang:python %}
class _EmptyClass(object):</p>

<pre><code>pass
</code></pre>

<p>def make_bisect_runner(parent, bisect_label):</p>

<pre><code>def split_tests(test_labels):
    """
    Split tests in half, but keep children together.
    """
    chunked_tests = defaultdict(list)
    for test_label in test_labels:
        cls_path = test_label.rsplit('.', 1)[0]
        # filter out our bisected test
        if test_label.startswith(bisect_label):
            continue
        chunked_tests[cls_path].append(test_label)

    chunk_a = []
    chunk_b = []
    midpoint = len(chunked_tests) / 2
    for n, cls_path in enumerate(chunked_tests):
        if n &lt; midpoint:
            chunk_a.extend(chunked_tests[cls_path])
        else:
            chunk_b.extend(chunked_tests[cls_path])
    return chunk_a, chunk_b

class BisectTestRunner(parent.__class__):
    """
    Based on Django 1.3's bisect_tests, recursively splits all tests that are discovered
    into a bisect grid, grouped by their parent TestCase.
    """
    # TODO: potentially break things down further than class level based on whats happening
    # TODO: the way we determine "stop" might need some improvement
    def run(self, test):
        # find all test_labels grouped by base class
        test_labels = []
        context_list = list(test._tests)
        while context_list:
            context = context_list.pop()
            if isinstance(context, unittest.TestCase):
                test = context.test
                test_labels.append('%s:%s.%s' % (test.__class__.__module__, test.__class__.__name__,
                                                 test._testMethodName))
            else:
                context_list.extend(context)

        subprocess_args = [sys.executable, sys.argv[0]] + [x for x in sys.argv[1:] if (x.startswith('-') and not x.startswith('--bisect'))]
        iteration = 1
        result = self._makeResult()
        test_labels_a, test_labels_b = [], []
        while True:
            chunk_a, chunk_b = split_tests(test_labels)
            if test_labels_a[:-1] == chunk_a and test_labels_b[:-1] == chunk_b:
                print "Failure found somewhere in", test_labels_a + test_labels_b
                break

            test_labels_a = chunk_a + [bisect_label]
            test_labels_b = chunk_b + [bisect_label]
            print '***** Pass %da: Running the first half of the test suite' % iteration
            print '***** Test labels:',' '.join(test_labels_a)
            failures_a = subprocess.call(subprocess_args + test_labels_a)

            print '***** Pass %db: Running the second half of the test suite' % iteration
            print '***** Test labels:',' '.join(test_labels_b)
            print
            failures_b = subprocess.call(subprocess_args + test_labels_b)

            if failures_a and not failures_b:
                print "***** Problem found in first half. Bisecting again..."
                iteration = iteration + 1
                test_labels = test_labels_a[:-1]
            elif failures_b and not failures_a:
                print "***** Problem found in second half. Bisecting again..."
                iteration = iteration + 1
                test_labels = test_labels_b[:-1]
            elif failures_a and failures_b:
                print "***** Multiple sources of failure found"
                print "***** test labels were:", test_labels_a[:-1] + test_labels_b[:-1]
                result.addError(test, (Exception, 'Failures found in multiple sets: %s and %s' % (test_labels_a[:-1], test_labels_b[:-1]), None))
                break
            else:
                print "***** No source of failure found..."
                break
        return result

inst = _EmptyClass()
inst.__class__ = BisectTestRunner
inst.__dict__.update(parent.__dict__)
return inst
</code></pre>

<p>class BisectTests(Plugin):</p>

<pre><code>def options(self, parser, env):
    parser.add_option("--bisect", dest="bisect_label", default=False)

def configure(self, options, config):
    self.enabled = bool(options.bisect_label)
    self.bisect_label = options.bisect_label

def prepareTestRunner(self, test):
    return make_bisect_runner(test, self.bisect_label)
</code></pre>

<p>{% endcodeblock %}</p>

<h2>Improvements to django-nose</h2>




<p>Finally I wanted to talk about some of the things that we've been pushing back upstream. The first was support for discovery of models that were in non-app tests. This works the same way as Django in that it looks for <code>appname/models.py</code>, and if it's found, it adds it to the <code>INSTALLED_APPS</code> automatically.</p>




<p>The second addition we've been working on allows you to run selective tests that dont require the database, and avoids actually building the database. It does this by looking for classes which inherit from <code>TransactionTestCase</code>, and if none are found, it skips database creation.</p>




<p>I'm curious to here what others have for tips and tricks regarding Nose (or maybe just helpful strategies in your own test runner).</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python and OS X Lion]]></title>
    <link href="http://justcramer.com/2011/07/20/python-and-os-x-lion"/>
    <updated>2011-07-20T00:00:00-07:00</updated>
    <id>http://justcramer.com/2011/07/20/python-and-os-x-lion</id>
    <content type="html"><![CDATA[<p>Just a few quick tips that I've had to run through and discover today while upgrading to Lion.</p>




<p>Start by <strong>installing Xcode 4</strong>, which is available via the App Store (for free now). This will fix your missing distutils package (which probably fixes a majority of your issues). You'll also need to <strong>reinstall all global site-packages</strong>, such as pip or virtualenvwrapper.</p>




<p>The last one, which was luckily solved for me already, was hitting <strong>[Errno 32] Broken pipe</strong> on various things. One example was this:</p>




<pre>  File "/Users/dcramer/.virtualenvs/disqus/lib/python2.6/site-packages/compress/utils.py", line 145, in filter_js
    return filter_common(js, verbosity, filters=settings.COMPRESS_JS_FILTERS, attr='filter_js', separator='', signal=js_filtered)
  File "/Users/dcramer/.virtualenvs/disqus/lib/python2.6/site-packages/compress/utils.py", line 136, in filter_common
    output = getattr(get_class(f)(verbose=(verbosity >= 2)), attr)(output)
  File "/Users/dcramer/.virtualenvs/disqus/lib/python2.6/site-packages/compress/filters/yui/__init__.py", line 41, in filter_js
    return self.filter_common(js, 'js', JS_ARGUMENTS)
  File "/Users/dcramer/.virtualenvs/disqus/lib/python2.6/site-packages/compress/filters/yui/__init__.py", line 20, in filter_common
    p.stdin.write(content)
TemplateSyntaxError: Caught IOError while rendering: [Errno 32] Broken pipe</pre>




<p>It turns out that with Xcode 4 there were some changes to the way (something that I dont care about) is handled. To solve this, add the following to your .profile:</p>




<pre>export ARCHFLAGS='-arch i386 -arch x86_64'</pre>




<p>If you rely on <a href="https://github.com/apenwarr/sshuttle">sshuttle</a> be warned, it doesn't work currently on OS X Lion.</p>




<p>I'll update this post if I hit any more issues, but so far everything else seems to be running smoothly.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[EuroPython]]></title>
    <link href="http://justcramer.com/2011/06/24/europython"/>
    <updated>2011-06-24T00:00:00-07:00</updated>
    <id>http://justcramer.com/2011/06/24/europython</id>
    <content type="html"><![CDATA[<p>This last week I've been attending <a href="http://europython.eu">EuroPython</a> over here in Firenze (or as we Americans know it, Florence), Italy. It's been a pretty amazing time, visiting the beautiful city, putting faces to names, and seeing some great presentations. More importantly, and the main reason for my trip, was the two talks that I delivered here this week.</p>




<p>The first was on Tuesday morning, titled "<strong><a href="http://www.slideshare.net/zeeg/building-scalable-web-apps" title="Building Scalable Web Apps">Building Scalable Web Apps</a></strong>". I tried to show how one might solve some problems building a real web app, so we built a little bit of a backend for a Twitter-like stream.</p>




<p style="width:425px;" id="__ss_8376349"> <iframe src="http://www.slideshare.net/slideshow/embed_code/8376349" width="425" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe> </p>




<p>I gave a second talk on Wednesday morning, "<strong><a href="http://www.slideshare.net/zeeg/pitfalls-of-continuous-deployment" title="Pitfalls of Continuous Deployment">Pitfalls of Continuous Deployment</a></strong>", which talks a little bit about the lessons we've learned during adoption of CD, as well as the value of integration and reporting systems</p>




<p style="width:425px" id="__ss_8386947"> <iframe src="http://www.slideshare.net/slideshow/embed_code/8386947" width="425" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe> </p>

]]></content>
  </entry>
  
</feed>
