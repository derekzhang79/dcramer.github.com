<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: disqus | David Cramer's Blog]]></title>
  <link href="http://justcramer.com/categories/disqus/atom.xml" rel="self"/>
  <link href="http://justcramer.com/"/>
  <updated>2012-05-04T15:55:09-07:00</updated>
  <id>http://justcramer.com/</id>
  <author>
    <name><![CDATA[David Cramer]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Distributing Work in Python Without Celery]]></title>
    <link href="http://justcramer.com/2012/05/04/distributing-work-without-celery"/>
    <updated>2012-05-04T15:12:00-07:00</updated>
    <id>http://justcramer.com/2012/05/04/distributing-work-without-celery</id>
    <content type="html"><![CDATA[<p>We've been migrating a lot of data to various places lately at DISQUS. These generally have been things like running
consistancy checks on our PostgreSQL shards, or creating a new system which requires a certain form of denormalized data. It
usually involves iterating through the results of an entire table (and sometimes even more), and performing some action
based on that row. We never care about results, we just want to be able to finish as quickly as possible.</p>




<p>Generally, we'd just create a simple <code>do_something.py</code> that would look something like this:</p>


<p>{% codeblock lang:python %}
for comment in RangeQuerySetWrapper(Post.objects.all()):</p>

<pre><code>do_something(comment)
</code></pre>

<p>{% endcodeblock %}</p>

<p>Note: RangeQuerySetWrapper is a wrapper around Django's ORM that efficiently iterates a table.</p>




<p>Eventually we came up with an internal tool to make this a bit more bearable. Mostly to handle resuming processes based
on the last primary key, and to track status. It evolved into a slightly more complex, but still simple utility we called
Taskmaster:</p>


<p>{% codeblock lang:python %}
def callback(obj):</p>

<pre><code>do_something(obj)
</code></pre>

<p>def main(**options):</p>

<pre><code>qs = Post.objects.all()
tm = Taskmaster(callback, qs, **options)
tm.start()
</code></pre>

<p>{% endcodeblock %}</p>

<p>This used to never be much of a problem. We'd just spin up some utility server and max the CPUs on that single machine
to get data processed in a day or less. Lately however, we've grown beyond the bounds of what is reasonable for a single
machine to take care of, and we've had to look towards other solutions.</p>




<p>As with most people, we rely on Celery and RabbitMQ for distributing asyncrhonous tasks in our application. Unfortunately
that's not quite the ideal fit out of the box for us in these situations. The root of the problem stems from the fact that
we may need to run through a billion objects, and without some effort, that would mean every single task would need to
fit into a RabbitMQ instance.</p>




<p>Given that we can't simply queue every task and then distribute them to some Celery workers, and even more so that we
simply dont want to bring up Celery machines/write throwaway Celery code for a simple script, we chose to take a different
route. That route ended up with a simple distributed buffer queue, built on the 
<a href="http://docs.python.org/library/multiprocessing.html">Python multiprocessing module</a>.</p>




<h3>Introducing Taskmaster</h3>




<p><a href="https://github.com/dcramer/taskmaster">Taskmaster</a> takes advantage of the remote management capabilities built into the multiprocessing module. This makes it
very simple to just throw in a capped Queue and have workers connect, get and execute jobs, and control state via that
single master process. In the end, we came up with an API looking something like this:</p>


<p>{% codeblock lang:bash %}</p>

<h1>spawn the master process</h1>

<p>$ tm-master taskmaster.example --reset --key=foo --host=0.0.0.0:5050</p>

<h1>run a slave</h1>

<p>$ tm-slave do_something:handle_job --host=192.168.0.1:5050
{% endcodeblock %}</p>

<p>You'll see the status on the master as things process, and if you cancel the process and start it again, it will
automatically resume:</p>


<p>{% codeblock lang:bash %}
$ tm-master taskmaster.example --reset --key=foo --host=0.0.0.0:5050
Taskmaster server running on '0.0.0.0:5050'
Current Job: 30421 | Rate:  991.06/s | Elapsed Time: 0:00:40
{% endcodeblock %}</p>

<p>Implementing the iterator and the callback are just as simple as they used to be:</p>


<p>{% codeblock lang:python %}
def get_jobs(last=0):</p>

<pre><code># ``last`` will only be passed if previous state was available
for obj in RangeQuerySetWrapper(Post.objects.all(), min_id=last):
    yield obj
</code></pre>

<p>def handle_job(obj):</p>

<pre><code>print "Got %r!" % obj
</code></pre>

<p>{% endcodeblock %}</p>

<p>Now under the hood Taskmaster will continue to iterate on <code>get_jobs</code> whenever the size of the queue is
under the threshold (which defaults to 10,000 items). This means we have a constant memory footprint and can just spin
slaves to process the data.</p>




<p>Taskmaster is still new, but if you're in need of these kinds of one-off migration scripts, we encourage you to <a href="https://github.com/dcramer/taskmaster">try
it out</a> and see if it fits.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Travis-CI with Python and Django]]></title>
    <link href="http://justcramer.com/2012/05/03/using-travis-ci"/>
    <updated>2012-05-03T11:13:00-07:00</updated>
    <id>http://justcramer.com/2012/05/03/using-travis-ci</id>
    <content type="html"><![CDATA[<p>I've been using <a href="http://travis-ci.org">Travis-CI</a> for a while now. Both my personal projects,
and even several of the libraries we maintain at DISQUS rely on it for Continuous Integration. I figured it was about time to confess
my undenying love for Travis, and throw up some notes about the defaults we use in our projects.</p>




<p>Getting started with Travis-CI is pretty easy. It involves putting a <code>.travis.yml</code> file in the root of
your project, and configuring the hooks between GitHub and Travis. While it's not always easy to get the hooks configured
when you're using organizations, I'm not going to talk much about that. What I do want to share is how we've structured
our configuration files for our Django and Python projects.</p>




<p>A basic <code>.travis.yml</code> might look something like this:</p>


<p>{% codeblock lang:yaml %}
language: python
python:
  - "2.6"
  - "2.7"
install:
  - pip install -q -e . --use-mirrors
script:
  - python setup.py test
{% endcodeblock %}</p>

<p>Most of the projects themselves use Django, which also means they need to test several Django versions. Travis makes
this very simple with its matrix builds. In our case, we need to setup a DJANGO matrix, and ensure it gets installed:</p>


<p>{% codeblock lang:yaml %}
env:
  - DJANGO=1.2.7
  - DJANGO=1.3.1
  - DJANGO=1.4
install:
  - pip install -q Django==$DJANGO --use-mirrors
  - pip install -q -e . --use-mirrors
{% endcodeblock %}</p>

<p>Additionally we generally conform to pep8, and we always want to run pyflakes against our build. We also use a custom
version of pyflakes which allows us to filter out warnings, as those are never critical errors. Add this in is pretty
simple using the <code>before_script</code> hook, which gets run before the tests are run in <code>script</code>.</p>


<p>{% codeblock lang:yaml %}
install:
  - pip install -q Django==$DJANGO --use-mirrors
  - pip install pep8 --use-mirrors
  - pip install https://github.com/dcramer/pyflakes/tarball/master
  - pip install -q -e . --use-mirrors
before_script:
  - "pep8 --exclude=migrations --ignore=E501,E225 src"
  - pyflakes -x W src
{% endcodeblock %}</p>

<p>When all is said and done, we end up with something like this:</p>


<p>{% codeblock lang:yaml %}
language: python
python:
  - "2.6"
  - "2.7"
env:
  - DJANGO=1.2.7
  - DJANGO=1.3.1
  - DJANGO=1.4
install:
  - pip install -q Django==$DJANGO --use-mirrors
  - pip install pep8 --use-mirrors
  - pip install https://github.com/dcramer/pyflakes/tarball/master
  - pip install -q -e . --use-mirrors
before_script:
  - "pep8 --exclude=migrations --ignore=E501,E225 src"
  - pyflakes -x W src
script:
  - python setup.py test
{% endcodeblock %}</p>

<p>Travis will automatically matrix each environment variable with each Python version, so you'll get
a test run for every combination of the two. Pretty easy, right?</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sticking With Standards]]></title>
    <link href="http://justcramer.com/2012/04/24/sticking-with-standards"/>
    <updated>2012-04-24T22:23:00-07:00</updated>
    <id>http://justcramer.com/2012/04/24/sticking-with-standards</id>
    <content type="html"><![CDATA[<p>More and more I'm seeing the "requirements.txt pattern" come up. This generally refers to projects (but not just), and
seems to have started around the same time as Heroku adopting Python. I feel like this is something that matters in the
Python world, and because I have an opinion on everything, I want to share mine.</p>




<h3>requirements.txt</h3>




<p>Let's first talk about what this pattern actually is. As you should already be familiar with pip (if you're not, this
post is not for you), the idea of this is that whatever you're doing, is installable by pointing pip at a requirements.txt
file which contains a list of your projects dependencies. This has some obvious benefits, one being that you can
mark repositories as dependencies.</p>




<p>Another benefit of this is when you have a large project (like DISQUS) and your dependencies can vary between environments. For
example, we have several various requirements files for disqus-web (our largest package):</p>




<pre>
requirements/global.txt
requirements/production.txt
requirements/development.txt
</pre>




<p>These end up being pretty obvious, and when an app has specific needs there's no reason not to approach the problem this
way. That said, you dont <strong>need</strong> to do things this way, and in every project other than our main repository,
including our open source work, all dependencies are specified completely in setup.py. Even in this case, we could just
as easily specify our core requirements as part of the package and simply have additional files which label the production
and development dependencies.</p>




<h3>setup.py is the right choice</h3>




<p>A common argument for not using setup.py is that a library is not the same as an app (or larger project). Why not? We
employ the same metadata in everything. Each contains a list of dependencies, some various metadata, and possibly a list
of extra resources (such as scripts, or documentation). Fundamentally they're identical. Additionally, if pip is your
thing, it <strong>does not prevent you from using setup.py</strong>. Let's take an example setup.py:</p>


<p>{% codeblock lang:python %}
from setuptools import setup, find_packages</p>

<p>requires = [</p>

<pre><code>'Flask==0.8',
'redis==2.4.11',
'hiredis==0.1.1',
'nydus==0.8.1',
</code></pre>

<p>]</p>

<p>setup(</p>

<pre><code>name='something-sexy',
version='1.0.0',
author="DISQUS",
author_email="dev@disqus.com",
package_dir={'': 'src'},
packages=find_packages("src"),
install_requires=requires,
zip_safe=False,
</code></pre>

<p>)
{% endcodeblock %}</p>

<p>Now, in our case, this is probably a service on Disqus, which means we're not listing it as a dependancy. In every
single scenario we have, we want our package to be on <code>PYTHONPATH</code>, and this is no different. There's many ways
to solve the problem, and generally adjusting <code>sys.path</code> is not what you're going to want. Whether you install
the package or you just run it as an editable package (via pip install -e or setuptool's develop command), packaging
your app makes it that much easier.</p>




<p>What's even more important is that you <strong>stick with standards</strong>, especially in our growing ecosystem of
open source and widely available libraries. There's absolutely no reason to have to explain to a developer that they
need to run some arbitrary command to get your neat tool to install. Following the well defined and adopted standards
ensures that is never the case.</p>




<p>Keep it simple. Keep it obvious.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Arrays as Materialized Paths in Postgres]]></title>
    <link href="http://justcramer.com/2012/04/08/using-arrays-as-materialized-paths-in-postgres"/>
    <updated>2012-04-08T16:52:00-07:00</updated>
    <id>http://justcramer.com/2012/04/08/using-arrays-as-materialized-paths-in-postgres</id>
    <content type="html"><![CDATA[<p>Something we've been casually working on at Disqus for <a href="http://justcramer.com/2010/05/30/scaling-threaded-comments-on-django-at-disqus/">quite some time</a> is an improved pagination method for threaded comments. This is obviously pretty important to us, it drives the very foundation of our product. It also happens to be an area that's somewhat challenging, and has a <a href="http://en.wikipedia.org/wiki/Nested_intervals">wide</a> <a href="http://en.wikipedia.org/wiki/Nested_set_model">array</a> <a href="http://en.wikipedia.org/wiki/Adjacency_list">of</a> <a href="https://communities.bmc.com/communities/docs/DOC-9902">solutions</a>. In the end, this is an overly complicated solution to solve the problem of threads having 10s or 100s of thousands of comments.</p>




<p>For some background, our first implementation is very similar to how <a href="http://reddit.com">Reddit</a> and many other systems work. It generally looks something like this:</p>




<ol>
    <li>Fetch all children for a tree</li>
    <li>Resort them in memory</li>
    <li>Return the N entry result set</li>
</ol>




<p>While fairly easy to implement, this has the enormous cost of pulling down every single child and resorting it at an application level. There are various ways to optimize this, and we even attempted doing it <a href="http://justcramer.com/2010/05/30/scaling-threaded-comments-on-django-at-disqus/">within the database</a> itself. In the end, none of our solutions worked at scale. They would either be too write heavy, or they'd move too much of the logic (read: CPU usage) to the database servers. That said, they led to something great, and in the end we settled on a solution that's neither too write or read heavy. That solution was materialized paths, but not in your typical way.</p>




<p>A materialized path generally is represented as a serialization of all parents. So in a simple case, it might be a simple delimited list of id values. As an example, let's say that we have a list of comments that are guaranteed to only be less than 1000 for their identifying value:</p>




<pre>
001
001002
001002003
001002007
001004
001005
001005006
</pre>




<p>In this case we've managed to stuff all of this into a sortable numeric value. Unfortunately, in the real world, it's never this easy, so we looked for existing solutions to solve this problem. We'll skip all of the bikeshedding here, and jump straight to our solution: Arrays.</p>




<p>Arrays are quite an interesting feature in Postgresql. They're a native data type, indexable, sortable, and contain a variety of operators and functions (and even more so in 8.4+). They also fit in nicely with our previous solution, with the caveat that we had to write to the arrays rather than generate them at execution time. In fact, they fit so well that we were able to directly translate a majority of the effort we spent while toying with CTEs.</p>




<p>What we finally settled on was a schema which looks something like this:</p>




<pre>
\d postsort

  Column   |   Type    | Modifiers 
-----------+-----------+-----------
 tree_id   | integer   | not null
 child_id  | integer   | not null
 value     | numeric[] | not null

Indexes:
    "postsort_pkey" PRIMARY KEY, btree (tree_id, child_id)
    "postsort_path" btree (tree_id, value)
</pre>




<p>A simple three-column schema gives us:</p>




<ul>
    <li><code>tree_id</code> The root node for this tree (for us, this is a comment thread)</li>
    <li><code>child_id</code> A child contained within this tree. There's a row for every child</li>
    <li><code>value</code> Our materialized path, implemented as an array</li>
</ul>




<p>The most important bit here is the <code>value</code>, and even more so what that array contains. Let's take a look at our previous example of simple numeric IDs, and how that'd be represented in this table:</p>




<pre>
child_id | value
----------------
1        | [1.0]
2        | [1.0, 2.0]
3        | [1.0, 2.0, 3.0]
7        | [1.0, 2.0, 7.0]
4        | [1.0, 4.0]
5        | [1.0, 5.0]
6        | [1.0, 5.0, 6.0]
</pre>




<p>You'll notice that the value always contains the id of the child as the last element, and is prefixed parents value. The child's ID <strong>must</strong> be present in order to guarantee sortability in conditions where these values are not unique. More specifically, in a real world scenario, you'll probably have some kind of <code>score</code> that you'd be including. As a demonstration of this eventual conflict, take the following values:</p>




<pre>
child_id | value
----------------
1        | [0.9134834, 1.0]
2        | [0.9134834, 1.0, 0.149341, 2.0]
3        | [0.9134834, 1.0, 0.149341, 2.0, 0.14123434, 3.0]
4        | [0.9134834, 1.0, 0.149341, 2.0, 0.14123434, 7.0]
5        | [0.9134834, 1.0, 0.149341, 5.0]
6        | [0.9134834, 1.0, 0.149341, 5.0, 0.601343, 5.0]
</pre>




<p>You'll see that we had a conflicting score for two children. If we always include the <strong>unique identifying numeric value</strong> we'll never have to worry about rows shifting into parents which they're not a part of. You will also see that we've prefixed each child's value with the score. This again gives us the numeric sorting order which we're looking for and allows us to sort by any arbitrary score. This could be anything from a timestamp to a completely custom scoring algorithm based on something like up and down votes on a child.</p>




<p>The schema and data storage is pretty straightforward, the bigger challenge is actually implementing the logic in your application (or if you're insane, within SQL triggers). We end up with a mess of SQL statements, with a singular goal to bring everything down to an atomic, transactionless nature. As an example, creating a new child probably resemebles something like the following:</p>


<p>{% codeblock lang:sql %}
INSERT INTO postsort (</p>

<pre><code>tree_id,
child_id,
value
</code></pre>

<p>)
SELECT t2.tree_id,</p>

<pre><code>   %(child_id)d as child_id,
   (t2.value || %(value)s::numeric[]) as value
</code></pre>

<p>FROM postsort as t2
WHERE t2.tree_id = %(tree_id)d
  AND t2.child_id = %(parent_child_id)d
{% endcodeblock %}</p>

<p>Once you've populated the table, queries become amazingly simple:</p>


<p>{% codeblock lang:sql %}
SELECT child_id
FROM postsort
WHERE tree_id = %(tree_id)s
ORDER BY value
{% endcodeblock %}</p>

<p>What's even more cool, aside from a lot of custom SQL we had to create for this to work in Django, is the fact that we were able to easily prototype and implement arrays within the Django ORM:</p>


<p>{% codeblock lang:python %}
class NumericArrayField(models.Field):</p>

<pre><code>__metaclass__ = models.SubfieldBase

def db_type(self):
    return "numeric[]"

def get_prep_value(self, value):
    if value:
        value = map(float, value)
    return value

def to_python(self, value):
    if value:
        value = map(float, value)
    return value
</code></pre>

<p>{% endcodeblock %}</p>

<p>We've just begun rolling this out at Disqus, but our initial performance and capacity tests are showing great results. The flexibility of arrays has been amazingly helpful in this scenario, and has pushed us into a new direction in what we can do with SQL. Disqus reaches more than 700 million unique visitors across its platform, and as always, Postgres has stood its ground and will continue to be our primary datastore of choice.</p>




<p>If Disqus sounds interesting to you, and you think you're a good fit and we're looking for passionate people to <a href="http://disqus.com/jobs/">join our team</a>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scaling Schema Changes]]></title>
    <link href="http://justcramer.com/2011/11/10/scaling-schema-changes"/>
    <updated>2011-11-10T16:06:00-08:00</updated>
    <id>http://justcramer.com/2011/11/10/scaling-schema-changes</id>
    <content type="html"><![CDATA[<p>I frequently get asked how Disqus deals with schema changes. It's a fair question, since we operate a fairly large amount of servers, but I also tend to think the answer is somewhat obvious. So let's start with the problem of schema changes at scale (in PostgreSQL).</p>




<p>Generally you have some table, let's call it a profile (since people seem to enjoy changing those). Well today, a new service has launched called Twooter, and we want to denormalize the user's Twooter name into their profile. To do this we need to add a new field, <code>twooter_username</code>.</p>




<h2>DDL First</h2>




<p>The first thing we have to realize, is that <strong>everyone will not have <code>twooter_username</code></strong>. Now even if that weren't true, it needs to be to maintain compatibility, and efficiency. For us, this means that <strong>all additions must be made as NULLable columns</strong>. This means that the old code can stay in place whether the schema change has been made or not, and more importantly, NULLable ALTERs are <strong>much</strong> quicker in Postgres.</p>




<p>It's very important that the schema change is made <strong>before</strong> the application's new version is deployed. Ideally you want to do the change as soon as the schema is finalized. I'll talk more a bit about the reasons for that later.</p>




<h2>Application Changes</h2>




<p>The second thing we need to concern ourselves with is our application logic. As I said before you <strong>must</strong> do the DDL before deploying your code changes. For us, this means all <strong>DDL happens in a branch</strong>, and can be merged once the change is completed. I also mentioned that additions must be NULLable, which not only means we can do the schema change before updating our application, but we also ensure forwards <strong>and</strong> backwards compatibility.</p>




<p>In addition to waiting for the schema change to complete before deploying your application, some changes may require several other steps along the release process. As an example, maybe we already had <code>twooter_username</code> stored in a different table, and we were literally just moving it to optimize our data access. This happens with a two things:</p>




<ul>
    <li>A write-through cache in the application to ensure <strong>new</strong> data is stored.</li>
    <li>A backfill operation to ensure old data is stored (this also must be idempotent).</li>
</ul>




<p>Once we've taken care of the above steps, only then can we actually utilize read operations on this new data. What this generally means is multi-step process to add a new data pattern:</p>




<ol>
    <li>Perform DDL.</li>
    <li>Deploy write-through cache code.</li>
    <li>Run backfill operation.</li>
    <li>Run sanity checks (verify the data is correct, and exists).</li>
    <li>Deploy code which utilizes new data.</li>
</ol>




<h2>DDL on a Cluster</h2>




<p>I've mostly been talking about how we scale the application side (read: code) for our DDL changes, but it's also important to note how we do no-downtime schema changes. For this there are two important concepts we utilize: platform-wide read-only mode, and enough capacity to remove a node from the cluster. The last part is important: <strong>enough capacity to remove a node from the cluster</strong>.</p>




<p>Now let's say this <code>twooter_username</code> is going to be added to a table which is so large, that even a fast NULLable ALTER cannot be run in production. In this case we're actually going to need to swap out our master PG node to ensure we don't hit any downtime, or slowness while making these changes. This is where read-only mode comes into play. It looks something like this:</p>




<ol>
    <li>Take a slave out of the pool.</li>
    <li>Run DDL on slave.</li>
    <li>Put it back into the pool.</li>
    <li>(repeat on all slaves)</li>
    <li>Turn on read-only.</li>
    <li>Promote a slave to master.</li>
    <li>(repeat DDL operation on former-master)</li>
</ol>




<p>And that's all there is to it. I'd be curious to hear if anyone else is doing things differently.</p>

]]></content>
  </entry>
  
</feed>
