<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: disqus | David Cramer's Blog]]></title>
  <link href="http://justcramer.com/categories/disqus/atom.xml" rel="self"/>
  <link href="http://justcramer.com/"/>
  <updated>2012-04-24T23:37:13-07:00</updated>
  <id>http://justcramer.com/</id>
  <author>
    <name><![CDATA[David Cramer]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Sticking With Standards]]></title>
    <link href="http://justcramer.com/2012/04/24/sticking-with-standards"/>
    <updated>2012-04-24T22:23:00-07:00</updated>
    <id>http://justcramer.com/2012/04/24/sticking-with-standards</id>
    <content type="html"><![CDATA[<p>More and more I'm seeing the "requirements.txt pattern" come up. This generally refers to projects (but not just), and
seems to have started around the same time as Heroku adopting Python. I feel like this is something that matters in the
Python world, and because I have an opinion on everything, I want to share mine.</p>




<h3>requirements.txt</h3>




<p>Let's first talk about what this pattern actually is. As you should already be familiar with pip (if you're not, this
post is not for you), the idea of this is that whatever your'e doing, is installable by pointing pip at a requirements.txt
file which contains a list of your projects dependencies. This has some obvious benefits, one being that you can
mark repositories as dependencies.</p>




<p>Another benefit of this is when you have a large project (like DISQUS) and your dependencies can vary between environments. For
example, we have several various requirements files for disqus-web (our largest package):</p>




<pre>
requirements/global.txt
requirements/production.txt
requirements/development.txt
</pre>




<p>These end up being pretty obvious, and when an app has specific needs there's no reason not to approach the problem this
way. That said, you dont <strong>need</strong> to do things this way, and in every project other than our main repository,
including our open source work, all dependencies are specified completely in setup.py. Even in this case, we could just
as easily specify our core requirements as part of the package and simply have additional files which label the production
and development dependencies.</p>




<h3>setup.py is the right choice</h3>




<p>A common argument for not using setup.py is that a library is not the same as an app (or larger project). Why not? We
employ the same metadata in everything. Each contains a list of dependencies, some various metadata, and possibly a list
of extra resources (such as scripts, or documentation). Fundamentally they're identical. Additionally, if pip is your
thing, it <strong>does not prevent you from using setup.py</strong>. Let's take an example setup.py:</p>


<p>{% codeblock lang:python %}
from setuptools import setup, find_packages</p>

<p>requires = [</p>

<pre><code>'Flask==0.8',
'redis==2.4.11',
'hiredis==0.1.1',
'nydus==0.8.1',
</code></pre>

<p>]</p>

<p>setup(</p>

<pre><code>name='something-sexy',
version='1.0.0',
author="DISQUS",
author_email="dev@disqus.com",
package_dir={'': 'src'},
packages=find_packages("src"),
install_requires=requires,
zip_safe=False,
</code></pre>

<p>)
{% endcodeblock %}</p>

<p>Now, in our case, this is probably a service on Disqus, which means we're not listing it as a dependancy. In every
single scenario we have, we want our package to be on <code>PYTHONPATH</code>, and this is no different. Usually this
is done with setuptools, and the (albeit, somewhat broken) <code>develop</code> command. Even if you want to use pip,
that's not a problem:</p>




<pre>
pip install -e .
</pre>




<p>What's even more important is that you <strong>stick with standards</strong>, especially in our growing ecosystem of
open source and widely available libraries. There's absolutely no reason to have to explain to a developer that they
need to run some arbitrary command to get your neat tool to install. Following the well defined and adopted standards
ensures that is never the case.</p>




<p>Keep it simple. Keep it obvious.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Arrays as Materialized Paths in Postgres]]></title>
    <link href="http://justcramer.com/2012/04/08/using-arrays-as-materialized-paths-in-postgres"/>
    <updated>2012-04-08T16:52:00-07:00</updated>
    <id>http://justcramer.com/2012/04/08/using-arrays-as-materialized-paths-in-postgres</id>
    <content type="html"><![CDATA[<p>Something we've been casually working on at Disqus for <a href="http://justcramer.com/2010/05/30/scaling-threaded-comments-on-django-at-disqus/">quite some time</a> is an improved pagination method for threaded comments. This is obviously pretty important to us, it drives the very foundation of our product. It also happens to be an area that's somewhat challenging, and has a <a href="http://en.wikipedia.org/wiki/Nested_intervals">wide</a> <a href="http://en.wikipedia.org/wiki/Nested_set_model">array</a> <a href="http://en.wikipedia.org/wiki/Adjacency_list">of</a> <a href="https://communities.bmc.com/communities/docs/DOC-9902">solutions</a>. In the end, this is an overly complicated solution to solve the problem of threads having 10s or 100s of thousands of comments.</p>




<p>For some background, our first implementation is very similar to how <a href="http://reddit.com">Reddit</a> and many other systems work. It generally looks something like this:</p>




<ol>
    <li>Fetch all children for a tree</li>
    <li>Resort them in memory</li>
    <li>Return the N entry result set</li>
</ol>




<p>While fairly easy to implement, this has the enormous cost of pulling down every single child and resorting it at an application level. There are various ways to optimize this, and we even attempted doing it <a href="http://justcramer.com/2010/05/30/scaling-threaded-comments-on-django-at-disqus/">within the database</a> itself. In the end, none of our solutions worked at scale. They would either be too write heavy, or they'd move too much of the logic (read: CPU usage) to the database servers. That said, they led to something great, and in the end we settled on a solution that's neither too write or read heavy. That solution was materialized paths, but not in your typical way.</p>




<p>A materialized path generally is represented as a serialization of all parents. So in a simple case, it might be a simple delimited list of id values. As an example, let's say that we have a list of comments that are guaranteed to only be less than 1000 for their identifying value:</p>




<pre>
001
001002
001002003
001002007
001004
001005
001005006
</pre>




<p>In this case we've managed to stuff all of this into a sortable numeric value. Unfortunately, in the real world, it's never this easy, so we looked for existing solutions to solve this problem. We'll skip all of the bikeshedding here, and jump straight to our solution: Arrays.</p>




<p>Arrays are quite an interesting feature in Postgresql. They're a native data type, indexable, sortable, and contain a variety of operators and functions (and even more so in 8.4+). They also fit in nicely with our previous solution, with the caveat that we had to write to the arrays rather than generate them at execution time. In fact, they fit so well that we were able to directly translate a majority of the effort we spent while toying with CTEs.</p>




<p>What we finally settled on was a schema which looks something like this:</p>




<pre>
\d postsort

  Column   |   Type    | Modifiers 
-----------+-----------+-----------
 tree_id   | integer   | not null
 child_id  | integer   | not null
 value     | numeric[] | not null

Indexes:
    "postsort_pkey" PRIMARY KEY, btree (tree_id, child_id)
    "postsort_path" btree (tree_id, value)
</pre>




<p>A simple three-column schema gives us:</p>




<ul>
    <li><code>tree_id</code> The root node for this tree (for us, this is a comment thread)</li>
    <li><code>child_id</code> A child contained within this tree. There's a row for every child</li>
    <li><code>value</code> Our materialized path, implemented as an array</li>
</ul>




<p>The most important bit here is the <code>value</code>, and even more so what that array contains. Let's take a look at our previous example of simple numeric IDs, and how that'd be represented in this table:</p>




<pre>
child_id | value
----------------
1        | [1.0]
2        | [1.0, 2.0]
3        | [1.0, 2.0, 3.0]
7        | [1.0, 2.0, 7.0]
4        | [1.0, 4.0]
5        | [1.0, 5.0]
6        | [1.0, 5.0, 6.0]
</pre>




<p>You'll notice that the value always contains the id of the child as the last element, and is prefixed parents value. The child's ID <strong>must</strong> be present in order to guarantee sortability in conditions where these values are not unique. More specifically, in a real world scenario, you'll probably have some kind of <code>score</code> that you'd be including. As a demonstration of this eventual conflict, take the following values:</p>




<pre>
child_id | value
----------------
1        | [0.9134834, 1.0]
2        | [0.9134834, 1.0, 0.149341, 2.0]
3        | [0.9134834, 1.0, 0.149341, 2.0, 0.14123434, 3.0]
4        | [0.9134834, 1.0, 0.149341, 2.0, 0.14123434, 7.0]
5        | [0.9134834, 1.0, 0.149341, 5.0]
6        | [0.9134834, 1.0, 0.149341, 5.0, 0.601343, 5.0]
</pre>




<p>You'll see that we had a conflicting score for two children. If we always include the <strong>unique identifying numeric value</strong> we'll never have to worry about rows shifting into parents which they're not a part of. You will also see that we've prefixed each child's value with the score. This again gives us the numeric sorting order which we're looking for and allows us to sort by any arbitrary score. This could be anything from a timestamp to a completely custom scoring algorithm based on something like up and down votes on a child.</p>




<p>The schema and data storage is pretty straightforward, the bigger challenge is actually implementing the logic in your application (or if you're insane, within SQL triggers). We end up with a mess of SQL statements, with a singular goal to bring everything down to an atomic, transactionless nature. As an example, creating a new child probably resemebles something like the following:</p>


<p>{% codeblock lang:sql %}
INSERT INTO postsort (</p>

<pre><code>tree_id,
child_id,
value
</code></pre>

<p>)
SELECT t2.tree_id,</p>

<pre><code>   %(child_id)d as child_id,
   (t2.value || %(value)s::numeric[]) as value
</code></pre>

<p>FROM postsort as t2
WHERE t2.tree_id = %(tree_id)d
  AND t2.child_id = %(parent_child_id)d
{% endcodeblock %}</p>

<p>Once you've populated the table, queries become amazingly simple:</p>


<p>{% codeblock lang:sql %}
SELECT child_id
FROM postsort
WHERE tree_id = %(tree_id)s
ORDER BY value
{% endcodeblock %}</p>

<p>What's even more cool, aside from a lot of custom SQL we had to create for this to work in Django, is the fact that we were able to easily prototype and implement arrays within the Django ORM:</p>


<p>{% codeblock lang:python %}
class NumericArrayField(models.Field):</p>

<pre><code>__metaclass__ = models.SubfieldBase

def db_type(self):
    return "numeric[]"

def get_prep_value(self, value):
    if value:
        value = map(float, value)
    return value

def to_python(self, value):
    if value:
        value = map(float, value)
    return value
</code></pre>

<p>{% endcodeblock %}</p>

<p>We've just begun rolling this out at Disqus, but our initial performance and capacity tests are showing great results. The flexibility of arrays has been amazingly helpful in this scenario, and has pushed us into a new direction in what we can do with SQL. Disqus reaches more than 700 million unique visitors across its platform, and as always, Postgres has stood its ground and will continue to be our primary datastore of choice.</p>




<p>If Disqus sounds interesting to you, and you think you're a good fit and we're looking for passionate people to <a href="http://disqus.com/jobs/">join our team</a>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scaling Schema Changes]]></title>
    <link href="http://justcramer.com/2011/11/10/scaling-schema-changes"/>
    <updated>2011-11-10T16:06:00-08:00</updated>
    <id>http://justcramer.com/2011/11/10/scaling-schema-changes</id>
    <content type="html"><![CDATA[<p>I frequently get asked how Disqus deals with schema changes. It's a fair question, since we operate a fairly large amount of servers, but I also tend to think the answer is somewhat obvious. So let's start with the problem of schema changes at scale (in PostgreSQL).</p>




<p>Generally you have some table, let's call it a profile (since people seem to enjoy changing those). Well today, a new service has launched called Twooter, and we want to denormalize the user's Twooter name into their profile. To do this we need to add a new field, <code>twooter_username</code>.</p>




<h2>DDL First</h2>




<p>The first thing we have to realize, is that <strong>everyone will not have <code>twooter_username</code></strong>. Now even if that weren't true, it needs to be to maintain compatibility, and efficiency. For us, this means that <strong>all additions must be made as NULLable columns</strong>. This means that the old code can stay in place whether the schema change has been made or not, and more importantly, NULLable ALTERs are <strong>much</strong> quicker in Postgres.</p>




<p>It's very important that the schema change is made <strong>before</strong> the application's new version is deployed. Ideally you want to do the change as soon as the schema is finalized. I'll talk more a bit about the reasons for that later.</p>




<h2>Application Changes</h2>




<p>The second thing we need to concern ourselves with is our application logic. As I said before you <strong>must</strong> do the DDL before deploying your code changes. For us, this means all <strong>DDL happens in a branch</strong>, and can be merged once the change is completed. I also mentioned that additions must be NULLable, which not only means we can do the schema change before updating our application, but we also ensure forwards <strong>and</strong> backwards compatibility.</p>




<p>In addition to waiting for the schema change to complete before deploying your application, some changes may require several other steps along the release process. As an example, maybe we already had <code>twooter_username</code> stored in a different table, and we were literally just moving it to optimize our data access. This happens with a two things:</p>




<ul>
    <li>A write-through cache in the application to ensure <strong>new</strong> data is stored.</li>
    <li>A backfill operation to ensure old data is stored (this also must be idempotent).</li>
</ul>




<p>Once we've taken care of the above steps, only then can we actually utilize read operations on this new data. What this generally means is multi-step process to add a new data pattern:</p>




<ol>
    <li>Perform DDL.</li>
    <li>Deploy write-through cache code.</li>
    <li>Run backfill operation.</li>
    <li>Run sanity checks (verify the data is correct, and exists).</li>
    <li>Deploy code which utilizes new data.</li>
</ol>




<h2>DDL on a Cluster</h2>




<p>I've mostly been talking about how we scale the application side (read: code) for our DDL changes, but it's also important to note how we do no-downtime schema changes. For this there are two important concepts we utilize: platform-wide read-only mode, and enough capacity to remove a node from the cluster. The last part is important: <strong>enough capacity to remove a node from the cluster</strong>.</p>




<p>Now let's say this <code>twooter_username</code> is going to be added to a table which is so large, that even a fast NULLable ALTER cannot be run in production. In this case we're actually going to need to swap out our master PG node to ensure we don't hit any downtime, or slowness while making these changes. This is where read-only mode comes into play. It looks something like this:</p>




<ol>
    <li>Take a slave out of the pool.</li>
    <li>Run DDL on slave.</li>
    <li>Put it back into the pool.</li>
    <li>(repeat on all slaves)</li>
    <li>Turn on read-only.</li>
    <li>Promote a slave to master.</li>
    <li>(repeat DDL operation on former-master)</li>
</ol>




<p>And that's all there is to it. I'd be curious to hear if anyone else is doing things differently.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Integrating Django with Nose at DISQUS]]></title>
    <link href="http://justcramer.com/2011/08/05/extending-django-nose"/>
    <updated>2011-08-05T00:00:00-07:00</updated>
    <id>http://justcramer.com/2011/08/05/extending-django-nose</id>
    <content type="html"><![CDATA[<p>About a month ago we decided to make the transition off of Django's test suite over to the Nose runners. Our main selling point was the extensibility, and the existing ecosystem of plugins. Four weeks later I'm happy to say we're running (basically) Nose with some minor extensions, and it's working great.</p>




<p>Getting Django running on Nose is no small feat. Luckily, someone else has already put in a lot of that effort, and packaged it up all nice and neat as <a href="http://pypi.python.org/pypi/django-nose">django-nose</a>. I won't go through setting up the package, but it's pretty straight forward. One thing that we quickly noticed however, was that it didnt quite fit our approach to testing, which was strictly unittest. After a couple days of going back and forth with some minor issues, we came up with a few pretty useful extensions to the platform.</p>




<p>A few of the big highlights for us:</p>




<ul>
    <li>Xunit integration (XML output of test results)</li>
    <li>Skipped and deprecated test hooks</li>
    <li>The ability to organize tests outside of the Django standards</li>
</ul>




<p>I'm wanted to talk a bit about how we solved some of our problems, and the other benefits we've seen since adopting it.</p>




<h3>Test Organization</h3>




<p>The biggest win for us was definitely being able to reorganize our test suite. This took a bit of work, and I'll talk about this with some of the plugins we whipped up to solve the problems. We ended up with a nice extensible test structure, similar to Django's own test suite:</p>




<pre>
tests/
tests/db/
tests/db/connections/
tests/db/connections/redis/
tests/db/connections/redis/__init__.py
tests/db/connections/redis/models.py
tests/db/connections/redis/tests.py
</pre>




<p>We retained the ability to keep tests within the common <code>app/tests</code> convention, but we found that we were just stuffing too many tests into obscure application paths that it became unmaintainable after a while.</p>


<h3>Unittest Compatibility</h3>




<p>The first issue we hit was with test discovery. Nose has a pretty good default pattern for finding tests, but it had some behavior that didn't quite fit with all of our existing code. Mostly, it found random functions that were prefixed with <code>test_</code>, or things like <code>start_test_server</code> which weren't tests by themselves.</p>




<p>After digging a bit into the API, it turned out to be a pretty easy problem to solve, and we came up with the following plugin:</p>


<p>{% codeblock lang:python %}
class UnitTestPlugin(object):</p>

<pre><code>"""
Enables unittest compatibility mode (dont test functions, only TestCase
subclasses, and only methods that start with [Tt]est).
"""
enabled = True

def wantClass(self, cls):
    if not issubclass(cls, unittest.TestCase):
        return False

def wantMethod(self, method):
    if not issubclass(method.im_class, unittest.TestCase):
        return False
    if not method.__name__.lower().startswith('test'):
        return False

def wantFunction(self, function):
    return False
</code></pre>

<p>{% endcodeblock %}</p>

<h2>Test Case Selection</h2>




<p>To ensure compatibility with our previous unittest extensions, we needed a simple way to filter only selenium tests. We do this with the --selenium and --exclude-selenium flags.</p>


<p>{% codeblock lang:python %}
from disqus.tests.testcases import DisqusSeleniumTest
from nose.plugins.base import Plugin</p>

<p>class SeleniumSelector(Plugin):</p>

<pre><code>def options(self, parser, env):
    parser.add_option("--exclude-selenium",
                      dest="selenium", action="store_false",
                      default=None)
    parser.add_option("--selenium",
                      dest="selenium", action="store_true",
                      default=None)

def configure(self, options, config):
    self.selenium = options.selenium
    self.enabled = options.selenium is not None

def wantClass(self, cls):
    if self.selenium:
        return issubclass(cls, DisqusSeleniumTest)
    elif issubclass(cls, DisqusSeleniumTest):
        return False
</code></pre>

<p>{% endcodeblock %}</p>

<h2>Bisecting Tests</h2>




<p>One feature I always thought was pretty useful in the Django test suite was their <code>--bisect</code> flag. Basically, given your test suite, and a failing test, it could help you find failures which were related to executing tests in say a specific order. This isn't actually made available to normal Django applications, but being a large codebase it's extremely useful for us.</p>




<p><strong>I should note, this one adapted from Django and is very rough. It doesn't report a proper <code>TestResult</code>, but it's pretty close to where we want to get it.</strong></p>


<p>{% codeblock lang:python %}
class _EmptyClass(object):</p>

<pre><code>pass
</code></pre>

<p>def make_bisect_runner(parent, bisect_label):</p>

<pre><code>def split_tests(test_labels):
    """
    Split tests in half, but keep children together.
    """
    chunked_tests = defaultdict(list)
    for test_label in test_labels:
        cls_path = test_label.rsplit('.', 1)[0]
        # filter out our bisected test
        if test_label.startswith(bisect_label):
            continue
        chunked_tests[cls_path].append(test_label)

    chunk_a = []
    chunk_b = []
    midpoint = len(chunked_tests) / 2
    for n, cls_path in enumerate(chunked_tests):
        if n &lt; midpoint:
            chunk_a.extend(chunked_tests[cls_path])
        else:
            chunk_b.extend(chunked_tests[cls_path])
    return chunk_a, chunk_b

class BisectTestRunner(parent.__class__):
    """
    Based on Django 1.3's bisect_tests, recursively splits all tests that are discovered
    into a bisect grid, grouped by their parent TestCase.
    """
    # TODO: potentially break things down further than class level based on whats happening
    # TODO: the way we determine "stop" might need some improvement
    def run(self, test):
        # find all test_labels grouped by base class
        test_labels = []
        context_list = list(test._tests)
        while context_list:
            context = context_list.pop()
            if isinstance(context, unittest.TestCase):
                test = context.test
                test_labels.append('%s:%s.%s' % (test.__class__.__module__, test.__class__.__name__,
                                                 test._testMethodName))
            else:
                context_list.extend(context)

        subprocess_args = [sys.executable, sys.argv[0]] + [x for x in sys.argv[1:] if (x.startswith('-') and not x.startswith('--bisect'))]
        iteration = 1
        result = self._makeResult()
        test_labels_a, test_labels_b = [], []
        while True:
            chunk_a, chunk_b = split_tests(test_labels)
            if test_labels_a[:-1] == chunk_a and test_labels_b[:-1] == chunk_b:
                print "Failure found somewhere in", test_labels_a + test_labels_b
                break

            test_labels_a = chunk_a + [bisect_label]
            test_labels_b = chunk_b + [bisect_label]
            print '***** Pass %da: Running the first half of the test suite' % iteration
            print '***** Test labels:',' '.join(test_labels_a)
            failures_a = subprocess.call(subprocess_args + test_labels_a)

            print '***** Pass %db: Running the second half of the test suite' % iteration
            print '***** Test labels:',' '.join(test_labels_b)
            print
            failures_b = subprocess.call(subprocess_args + test_labels_b)

            if failures_a and not failures_b:
                print "***** Problem found in first half. Bisecting again..."
                iteration = iteration + 1
                test_labels = test_labels_a[:-1]
            elif failures_b and not failures_a:
                print "***** Problem found in second half. Bisecting again..."
                iteration = iteration + 1
                test_labels = test_labels_b[:-1]
            elif failures_a and failures_b:
                print "***** Multiple sources of failure found"
                print "***** test labels were:", test_labels_a[:-1] + test_labels_b[:-1]
                result.addError(test, (Exception, 'Failures found in multiple sets: %s and %s' % (test_labels_a[:-1], test_labels_b[:-1]), None))
                break
            else:
                print "***** No source of failure found..."
                break
        return result

inst = _EmptyClass()
inst.__class__ = BisectTestRunner
inst.__dict__.update(parent.__dict__)
return inst
</code></pre>

<p>class BisectTests(Plugin):</p>

<pre><code>def options(self, parser, env):
    parser.add_option("--bisect", dest="bisect_label", default=False)

def configure(self, options, config):
    self.enabled = bool(options.bisect_label)
    self.bisect_label = options.bisect_label

def prepareTestRunner(self, test):
    return make_bisect_runner(test, self.bisect_label)
</code></pre>

<p>{% endcodeblock %}</p>

<h2>Improvements to django-nose</h2>




<p>Finally I wanted to talk about some of the things that we've been pushing back upstream. The first was support for discovery of models that were in non-app tests. This works the same way as Django in that it looks for <code>appname/models.py</code>, and if it's found, it adds it to the <code>INSTALLED_APPS</code> automatically.</p>




<p>The second addition we've been working on allows you to run selective tests that dont require the database, and avoids actually building the database. It does this by looking for classes which inherit from <code>TransactionTestCase</code>, and if none are found, it skips database creation.</p>




<p>I'm curious to here what others have for tips and tricks regarding Nose (or maybe just helpful strategies in your own test runner).</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[EuroPython]]></title>
    <link href="http://justcramer.com/2011/06/24/europython"/>
    <updated>2011-06-24T00:00:00-07:00</updated>
    <id>http://justcramer.com/2011/06/24/europython</id>
    <content type="html"><![CDATA[<p>This last week I've been attending <a href="http://europython.eu">EuroPython</a> over here in Firenze (or as we Americans know it, Florence), Italy. It's been a pretty amazing time, visiting the beautiful city, putting faces to names, and seeing some great presentations. More importantly, and the main reason for my trip, was the two talks that I delivered here this week.</p>




<p>The first was on Tuesday morning, titled "<strong><a href="http://www.slideshare.net/zeeg/building-scalable-web-apps" title="Building Scalable Web Apps">Building Scalable Web Apps</a></strong>". I tried to show how one might solve some problems building a real web app, so we built a little bit of a backend for a Twitter-like stream.</p>




<p style="width:425px;" id="__ss_8376349"> <iframe src="http://www.slideshare.net/slideshow/embed_code/8376349" width="425" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe> </p>




<p>I gave a second talk on Wednesday morning, "<strong><a href="http://www.slideshare.net/zeeg/pitfalls-of-continuous-deployment" title="Pitfalls of Continuous Deployment">Pitfalls of Continuous Deployment</a></strong>", which talks a little bit about the lessons we've learned during adoption of CD, as well as the value of integration and reporting systems</p>




<p style="width:425px" id="__ss_8386947"> <iframe src="http://www.slideshare.net/slideshow/embed_code/8386947" width="425" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe> </p>

]]></content>
  </entry>
  
</feed>
